import pandas as pd

table1 = {
   "product": ["Laptop","Phone","Laptop"],
   "price" : [60000,30000,20000],
   "brand" : ["Dell","Samsung","Apple"]
}
newdf = pd.DataFrame(table1)
newdf OR print(newdf)


##  Reading data 

import pandas as pd
sales_df = pd.read_csv("sales.csv")
print(df.head()) 

## df.head() --> first 5 rows
## df.tail() --> last 5 rows
## df.info() --> structure of the dataset
## df.describe() --> statistics for numeric columns


## df["column_name"] --> gives you series (column)
## df[["col1","col2"]] --> gives you a dataFrame with multiple columns 


Assume sales_df has a column called "price".

Write one line of code to select only that column from sales_df. ==== sales_df["price"]

Write one line that selects product and brand only. ==== sales_df[["product","brand"]]


df[df["age"] > 25] 
give me all rows where age is greater than 25.

df[df["city"] == "Delhi"]

sales_df[sales_df["price"]>30000]

df[(df["age"] > 25) & (df["city"] == "Delhi")]
Use () around each condition

Use & for AND    ] Never use and or or — pandas will complain.

Use | for OR

## ==> sales_df[(sales_df["price"] > 30000) & (sales_df["brand"] =="Dell")]

## Creating New Columns
df["age_in_5_years"] = df["age"] + 5

df["full_name"] = df["first_name"] + " " + df["last_name"]
df["discounted_price"] = df["price"] * 0.9




Write one line to create a new column called "price_with_tax"
== > sales_df["price_with_tax"] = sales_df["price"] * 1.18

# SORTING DATA

df.sort_values("price")
df.sort_values("price",ascending=False)
## sort by multiple columns 
df.sort_values(["brand","price"])

sales_df.sort_values("price",ascending=False)



## df[df["price"] > 30000][["product", "price"]]
Give me only rows where price > 30000,
and from those rows, show only product and price columns

sales_df[sales_df["brand"]=="Samsung"][["product","price"]]

## RESETTING THE INDEX 
filtered = df[df["price"]>30000]
filtered = filtered.reset_index(drop=True)
reset_index() gives you fresh row numbers

drop=True prevents the old index from becoming a new column

expensive = sales_df(sales_df["price"]>30000)
expensive = expensive.reset_index(drop=True)



#### loc is how you select rows + columns in a clean, explicit way.
df.loc[0]  # row by index
df.loc[0:5]  # row slice
df.loc[:,"price"] # all rows one column
df.loc[:,["product","bran"]] 
df.loc[df["price"]>30000],["product","price"]]
# loc == [row_selection,column_selection]

sales_df.loc[sales_df["price"]>30000,["product","price"]]
loc = select by labels
iloc = select by index positions

df.iloc[0]            # first row
df.iloc[0:3]          # first three rows
df.iloc[:, 0]         # all rows, first column
df.iloc[:, 0:2]       # all rows, first two columns
df.iloc[2, 1]         # row 2, column 1 (single cell)

sales_df.iloc[:,0:2]



### RENAMING 
df.rename(columns={"price": "unit_price", "brand": "company"}, inplace=True)
sales_df.rename(columns={"price":"unit_price"},inplace=True)


### Dropping column
df.drop("age", axis=1, inplace=True)
df.drop(["age","city"],axis =1 , inplace = True)
axis = 1 Drop columns
axis = 0 drop rows

sales_df.drop("brand",axis=1, inplace=True)


### Handling missing values, null values NaN
df.dropna() # drop the rows
df.fillna(0)  # fill the missing values
df.fillna("Unknown")

sales_df.fillna("Unknonwn",inplace=True) | sales_df = sales_df.fillna("Unknown")
With inplace=True, this extra assignment isn’t needed because the change happens in the original object.


## Values counts how many times each unique value appears
df["brand"].value_counts()
sales_df["product"].value_counts()
df["product"].unique()      # list of unique values
df["product"].nunique()     # count of unique values
sales_df["brand"].nunique()

nunique() tells you how many unique boxes exist
value_counts() tells you what’s inside each box and how much


groupby lets you answer questions like:
Total sales per product
Average price per brand
Count of orders per city
Anything “by category”

It’s like SQL’s GROUP BY, but smoother.
df.groupby("brand")["price"].mean() # avg Group rows by brand, then take the mean of price
df.groupby("product")["price"].sum()

sales_df.groupby("product")["unit_price"].mean()

# df.groupby("product")[["unit_price", "quantity"]].mean()
df.groupby("product").agg({
    "unit_price": "mean",
    "quantity": "sum"            } Multiple calculations
})

sales_df.groupby("product")["quantity"].sum()

sales_df.groupby("product").agg(
 {
   "unit_price": "mean",
   "quantity": "sum"
}
)

## df.groupby("product")["quantity"].sum().sort_values(ascending=False)
sales_df.groupby("product").agg(
 {
   "unit_price": "mean",
   "quantity": "sum"
}
).sort_values(by="quantity",ascending=False)